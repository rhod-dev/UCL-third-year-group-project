{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Label Classification of Question 4\n",
    "\n",
    "## List of contents\n",
    "\n",
    "- [Importing data](#import)\n",
    "- [NLP treatment of Q4 responses (x_train, x_test)](#nlp)\n",
    "- [Data Augmentation of Q4 responses (x_train, x_test)](#augment)\n",
    "- [Vectorisation of Q4 responses (x_train, x_test)](#vectorise)\n",
    "- [Multi One-hot Encoding of categories (y_train, y_test)](#onehot)\n",
    "- [Neural network model (Keras LSTM-Dense Model)](#model)\n",
    "    - Model structure\n",
    "    - Model train\n",
    "    - Model fit\n",
    "    - Model evaluate\n",
    "    - Model predict\n",
    "- [Accuracy analysis](#accuracyanalysis)\n",
    "    - [Precision | Recall | F1-measure for different thresholds](#f1)\n",
    "- [Automatic Categorisation of Unseen Data](#unseen)\n",
    "- [Function for precision, recall and f1score](#accuracygraph)\n",
    "- [Function for threshold and outputting categories](#categoryoutput)\n",
    "- [Testing with different parameters, thresholds and optimising](#optimise)\n",
    "    - [Run 1 - Gaussian Noise](#run1) \n",
    "    - [Run 2 - Batch Normalisation](#run2)\n",
    "    - [Run 3 - Kernel Initialiser](#run3)\n",
    "    - [Run 4 - Noise + Normalisation + Kernel Initialiser](#run4)\n",
    "    - [Run 5 - Normalisation + Kernel Initialiser](#run5)\n",
    "    - [Run 6 - Random search/Grid Search](#run6)\n",
    "    - [Run 7 - Changing the no of LSTM cells](#run7)\n",
    "    - [Run 8 - Embedding dimension](#run8)\n",
    "- [Final Model - Q4_categorise function](#finalmodel)\n",
    "- [Final Run on 2019 with category outputs](#final)\n",
    "- [Improvement - Data Augmentation](#augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\roddy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "## importing libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# nltk library \n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "# tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "# keras library \n",
    "   # preprocessing\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "   # model\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, Dropout, SpatialDropout1D\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import plot_model\n",
    "   # accuracy and optimisation\n",
    "from keras.layers import GaussianNoise\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#sklearn\n",
    "from sklearn.preprocessing import MultiLabelBinarizer # multi onehot encoding \n",
    "from sklearn.model_selection import train_test_split # train-test split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing data <a name=\"import\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to import datafiles\n",
    "\n",
    "def import_df(df):\n",
    "    '''This function imports csv files and is designed\n",
    "    to import files with unequal dataframes as well\n",
    "    input: df, csv file\n",
    "    output: datafile as pandas dataframe'''\n",
    "\n",
    "    # Delimiter\n",
    "    data_file_delimiter = ','\n",
    "\n",
    "    # The max column count a line in the file could have\n",
    "    largest_column_count = 0\n",
    "\n",
    "    # Loop the data lines\n",
    "    with open(df, 'r') as temp_f:\n",
    "        # Read the lines\n",
    "        lines = temp_f.readlines()\n",
    "\n",
    "        for l in lines:\n",
    "            # Count the column count for the current line\n",
    "            column_count = len(l.split(data_file_delimiter)) + 1\n",
    "\n",
    "            # Set the new most column count\n",
    "            largest_column_count = column_count if largest_column_count < column_count else largest_column_count\n",
    "\n",
    "    # Close file\n",
    "    temp_f.close()\n",
    "\n",
    "    # Generate column names (will be 0, 1, 2, ..., largest_column_count - 1)\n",
    "    column_names = [i for i in range(0, largest_column_count)]\n",
    "\n",
    "    # Read csv\n",
    "    datafile = pd.read_csv(df, header=None, delimiter=data_file_delimiter, names=column_names)\n",
    "    \n",
    "    return datafile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "q4_1617_eng = import_df('Q4_1617_Eng.csv')\n",
    "q4_1617_noteng = import_df('Q4_1617_NonEng.csv')\n",
    "\n",
    "q4_1718_eng = import_df('Q4_1718_Eng.csv')\n",
    "q4_1718_noteng = import_df('Q4_1718_NonEng.csv')\n",
    "\n",
    "q4_1819_eng = import_df('Q4_1819_Eng.csv')\n",
    "q4_1819_noteng = import_df('Q4_1819_NonEng.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the dataframe with blank responses is (453, 9)\n",
      "The shape of the dataframe for Q4 is (371, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Response</th>\n",
       "      <th>just1</th>\n",
       "      <th>category1</th>\n",
       "      <th>just2</th>\n",
       "      <th>category2</th>\n",
       "      <th>just3</th>\n",
       "      <th>category3</th>\n",
       "      <th>just4</th>\n",
       "      <th>category4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>to gain practical skills that may be used in t...</td>\n",
       "      <td>to gain practical skills that may be used in t...</td>\n",
       "      <td>US</td>\n",
       "      <td>gain data for analysis</td>\n",
       "      <td>TD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>to learn how to conduct physics experiments</td>\n",
       "      <td>to learn how to conduct physics experiments</td>\n",
       "      <td>UX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>To prove theoretical work and to just determin...</td>\n",
       "      <td>To prove theoretical work</td>\n",
       "      <td>TT</td>\n",
       "      <td>determine phenomena in particular not yet expl...</td>\n",
       "      <td>TR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>develop us into autonomous professionals</td>\n",
       "      <td>develop us into autonomous professionals</td>\n",
       "      <td>UI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>to learn how physicists think</td>\n",
       "      <td>to learn how physicists think</td>\n",
       "      <td>UP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>to prove something</td>\n",
       "      <td>to prove something</td>\n",
       "      <td>TT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>to collect data and investigate a hypothesis</td>\n",
       "      <td>to collect data and</td>\n",
       "      <td>TD</td>\n",
       "      <td>investigate a hypothesis</td>\n",
       "      <td>TT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>to illustrate/prove theories. To learn practic...</td>\n",
       "      <td>to illustrate/prove theories.</td>\n",
       "      <td>TT</td>\n",
       "      <td>To learn practical skills</td>\n",
       "      <td>US</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>Prove that a hypothesis is correct or wrong ac...</td>\n",
       "      <td>Prove that a hypothesis is correct or wrong ac...</td>\n",
       "      <td>TT</td>\n",
       "      <td>obtained results</td>\n",
       "      <td>TD</td>\n",
       "      <td>with a defendable method</td>\n",
       "      <td>UX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>test a theory</td>\n",
       "      <td>test a theory</td>\n",
       "      <td>TT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>371 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Response  \\\n",
       "1   to gain practical skills that may be used in t...   \n",
       "2         to learn how to conduct physics experiments   \n",
       "3   To prove theoretical work and to just determin...   \n",
       "4            develop us into autonomous professionals   \n",
       "5                       to learn how physicists think   \n",
       "..                                                ...   \n",
       "65                                 to prove something   \n",
       "66       to collect data and investigate a hypothesis   \n",
       "67  to illustrate/prove theories. To learn practic...   \n",
       "68  Prove that a hypothesis is correct or wrong ac...   \n",
       "69                                      test a theory   \n",
       "\n",
       "                                                just1 category1  \\\n",
       "1   to gain practical skills that may be used in t...        US   \n",
       "2         to learn how to conduct physics experiments        UX   \n",
       "3                          To prove theoretical work         TT   \n",
       "4            develop us into autonomous professionals        UI   \n",
       "5                       to learn how physicists think        UP   \n",
       "..                                                ...       ...   \n",
       "65                                 to prove something        TT   \n",
       "66                                to collect data and        TD   \n",
       "67                      to illustrate/prove theories.        TT   \n",
       "68  Prove that a hypothesis is correct or wrong ac...        TT   \n",
       "69                                      test a theory        TT   \n",
       "\n",
       "                                                just2 category2  \\\n",
       "1                              gain data for analysis        TD   \n",
       "2                                                 NaN       NaN   \n",
       "3   determine phenomena in particular not yet expl...        TR   \n",
       "4                                                 NaN       NaN   \n",
       "5                                                 NaN       NaN   \n",
       "..                                                ...       ...   \n",
       "65                                                NaN       NaN   \n",
       "66                           investigate a hypothesis        TT   \n",
       "67                          To learn practical skills        US   \n",
       "68                                   obtained results        TD   \n",
       "69                                                NaN       NaN   \n",
       "\n",
       "                       just3 category3 just4 category4  \n",
       "1                        NaN       NaN   NaN       NaN  \n",
       "2                        NaN       NaN   NaN       NaN  \n",
       "3                        NaN       NaN   NaN       NaN  \n",
       "4                        NaN       NaN   NaN       NaN  \n",
       "5                        NaN       NaN   NaN       NaN  \n",
       "..                       ...       ...   ...       ...  \n",
       "65                       NaN       NaN   NaN       NaN  \n",
       "66                       NaN       NaN   NaN       NaN  \n",
       "67                       NaN       NaN   NaN       NaN  \n",
       "68  with a defendable method        UX   NaN       NaN  \n",
       "69                       NaN       NaN   NaN       NaN  \n",
       "\n",
       "[371 rows x 9 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Year 16-17 Dataset ###\n",
    "# Eng\n",
    "dfq4_1617_eng = pd.DataFrame()\n",
    "dfq4_1617_eng = q4_1617_eng[1:]\n",
    "# Not Eng\n",
    "dfq4_1617_noteng = pd.DataFrame()\n",
    "dfq4_1617_noteng = q4_1617_noteng[1:]\n",
    "\n",
    "### Year 17-18 Dataset ###\n",
    "# Eng\n",
    "dfq4_1718_eng = pd.DataFrame()\n",
    "dfq4_1718_eng = q4_1718_eng[1:]\n",
    "# Not Eng\n",
    "dfq4_1718_noteng = pd.DataFrame()\n",
    "dfq4_1718_noteng = q4_1718_noteng[1:]\n",
    "\n",
    "### Year 18-19 Dataset ###\n",
    "# Eng\n",
    "dfq4_1819_eng = pd.DataFrame()\n",
    "dfq4_1819_eng = q4_1819_eng[1:]\n",
    "# Not Eng\n",
    "dfq4_1819_noteng = pd.DataFrame()\n",
    "dfq4_1819_noteng = q4_1819_noteng[1:]\n",
    "\n",
    "dfq4_all = pd.concat([dfq4_1617_eng,dfq4_1617_noteng,dfq4_1718_eng,dfq4_1718_noteng,dfq4_1819_eng,dfq4_1819_noteng])\n",
    "dfq4_all = dfq4_all.rename(columns={1:\"Response\",3:\"category1\",5:\"category2\",7:\"category3\",9:\"category4\",11:\"category5\",13:\"category6\",15:\"category7\"})\n",
    "\n",
    "# check null columns and remove null and unwanted columns\n",
    "# print(dfq4_all.category4.isnull().all())\n",
    "dfq4_all = dfq4_all.drop([0],axis=1)\n",
    "dfq4_all = dfq4_all.drop([10],axis=1)\n",
    "dfq4_all = dfq4_all.drop([12],axis=1)\n",
    "dfq4_all = dfq4_all.drop([14],axis=1)\n",
    "dfq4_all = dfq4_all.drop([16],axis=1)\n",
    "dfq4_all = dfq4_all.drop(['category5'],axis=1)\n",
    "dfq4_all = dfq4_all.drop(['category6'],axis=1)\n",
    "dfq4_all = dfq4_all.drop(['category7'],axis=1)\n",
    "\n",
    "dfq4_all = dfq4_all.rename(columns={2:'just1',4:'just2',6:'just3',8:'just4'})\n",
    "print('The shape of the dataframe with blank responses is',dfq4_all.shape)\n",
    "\n",
    "# drop blank responses from columns\n",
    "dfq4_all = dfq4_all[dfq4_all.category1 != 'BL']\n",
    "dfq4_all = dfq4_all[dfq4_all.category1 != 'bl']\n",
    "dfq4_all = dfq4_all[dfq4_all.category1 != 'b;']\n",
    "\n",
    "dfq4_all = dfq4_all.fillna('NaN')\n",
    "\n",
    "print('The shape of the dataframe for Q4 is',dfq4_all.shape)\n",
    "\n",
    "dfq4_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP treatment of responses - creating x_train and x_test <a name=\"nlp\"></a>\n",
    "To increase if NLP preprocessing increases the accuracy of the ML algorithm, we will treat the data and pass both our raw and cleaned versions to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RO's function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemming = PorterStemmer()\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "\n",
    "def clean_text(dirty_text):\n",
    "    '''This function works on a raw text string, and:\n",
    "        1) changes to lower case\n",
    "        2) tokenizes --> breaks down into words\n",
    "        3) removes punctuation and non-word text\n",
    "        4) finds word stems\n",
    "        5) removes stop words\n",
    "        6) rejoins meaningful stem words'''\n",
    "    \n",
    "    # Convert to lower case\n",
    "    #text = raw_text.lower()\n",
    "    #text = raw_text.applymap(lambda s:s.lower() if type(s) == float else s)\n",
    "\n",
    "    clean = []\n",
    "    for row in dirty_text:\n",
    "        # Tokenize\n",
    "        tokens = nltk.word_tokenize(str(row))\n",
    "    \n",
    "        # Keep only words (removes punctuation + numbers)\n",
    "        token_words = [w for w in tokens if w.isalpha()]\n",
    "    \n",
    "        # Stemming\n",
    "        stemmed_words = [stemming.stem(w) for w in token_words]\n",
    "    \n",
    "        # Remove stop words\n",
    "        meaningful_words = [w for w in stemmed_words if not w in stops]\n",
    "    \n",
    "        # Rejoin meaningful stemmed words\n",
    "        joined_words = ( \" \".join(meaningful_words))\n",
    "        \n",
    "        clean.append(joined_words)\n",
    "\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "q4response_raw = dfq4_all.Response.tolist()\n",
    "q4response_clean = clean_text(dfq4_all.Response.tolist())\n",
    "\n",
    "#print(q4response_raw)\n",
    "#print(q4response_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to difficulties with the code and time constraints, methods of data augmentation have been left hashed out and unused below. However, the methods and appropriate code with their sources have been left as legacy for future improvements to the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1st augmentation suite:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/opla/text-augmentation-for-machine-learning-tasks-how-to-grow-your-text-dataset-for-classification-38a9a207f88d\n",
    "\n",
    "Possible models:\n",
    "1. Synonym Replacement (SR): Randomly\n",
    "choose n words from the sentence that are not\n",
    "stop words. Replace each of these words with\n",
    "one of its synonyms chosen at random.\n",
    "2. Random Insertion (RI): Find a random synonym of a random word in the sentence that is\n",
    "not a stop word. Insert that synonym into a random position in the sentence. Do this n times.\n",
    "3. Random Swap (RS): Randomly choose two\n",
    "words in the sentence and swap their positions.\n",
    "Do this n times.\n",
    "4. Random Deletion (RD): Randomly remove\n",
    "each word in the sentence with probability p."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1: Word/sentence shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk import word_tokenize\n",
    "# import random\n",
    "\n",
    "# q4response_raw = dfq4_all.Response.tolist()\n",
    "# q4response_clean = clean_text(dfq4_all.Response.tolist())\n",
    "# aug1 = q4response_clean\n",
    "# aug2 = q4response_raw\n",
    "\n",
    "\n",
    "# def augment(sentence,n):\n",
    "#     new_sentences = []\n",
    "#     words = word_tokenize(sentence)\n",
    "#     for i in range(n):\n",
    "#         random.shuffle(words)\n",
    "#         new_sentences.append(' '.join(words))\n",
    "#     new_sentences = list(set(new_sentences))\n",
    "#     return new_sentences\n",
    "\n",
    "# c = augment(str(aug1),10)\n",
    "# print(c)\n",
    "# d = augment(str(aug2),10)\n",
    "# print(d)\n",
    "\n",
    "# # concantenate augmented data to original\n",
    "# for i in c : \n",
    "#     aug1.append(i) \n",
    "    \n",
    "# for i in d : \n",
    "#     aug2.append(i) \n",
    "  \n",
    "# # printing concatenated list\n",
    "# print (\"Concatenated clean list: \" + str(aug1)) \n",
    "\n",
    "# print (\"Concatenated clean list: \" + str(aug2)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: Synonym replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk import word_tokenize\n",
    "# from nltk.corpus import stopwords\n",
    "\n",
    "# stoplist = stopwords.words('english')\n",
    "\n",
    "\n",
    "# def get_synonyms_lexicon(path):\n",
    "#     synonyms_lexicon = {}\n",
    "#     # change below, alternatively errors='ignore' instead of encoding\n",
    "#     text_entries = [l.strip() for l in open(path, encoding = \"latin-1\").readlines()]\n",
    "#     for e in text_entries:\n",
    "#         e = e.split(' ')\n",
    "#         k = e[0]\n",
    "#         v = e[1:len(e)]\n",
    "#         synonyms_lexicon[k] = v\n",
    "#     return synonyms_lexicon\n",
    "\n",
    "\n",
    "# def synonym_replacement(sentence, synonyms_lexicon):\n",
    "#     keys = synonyms_lexicon.keys()\n",
    "#     words = word_tokenize(sentence)\n",
    "#     n_sentence = sentence\n",
    "#     for w in words:\n",
    "#         if w not in stoplist:\n",
    "#             if w in keys:\n",
    "#                 n_sentence = n_sentence.replace(w, synonyms_lexicon[w][0])  # we replace with the first synonym\n",
    "#     return n_sentence\n",
    "\n",
    "# #http://paraphrase.org/#/download\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     text = 'Many customers initiated a return process of the product as it was not suitable for use.' \\\n",
    "#            'It was conditioned in very thin box which caused scratches on the main screen.' \\\n",
    "#            'The involved firms positively answered their clients who were fully refunded.'\n",
    "#     sentences = text.split('.')\n",
    "#     sentences.remove('')\n",
    "#     print(sentences)\n",
    "#     synonyms_lexicon = get_synonyms_lexicon('./ppdb-2.0--all.gz')\n",
    "#     for sentence in sentences:\n",
    "#         new_sentence = synonym_replacement(sentence, synonyms_lexicon)\n",
    "#         print('%s' % sentence)\n",
    "#         print('%s' % new_sentence)\n",
    "#         print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2nd augmentation suite:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/makcedward/nlpaug#augmenter\n",
    "\n",
    "Uses independently created NLPAug \n",
    "\n",
    "Example of Textual Augmenter Usage:\n",
    "- Character Augmenter\n",
    "    - OCR\n",
    "    - Keyboard\n",
    "    - Random\n",
    "- Word Augmenter\n",
    "    - Spelling\n",
    "    - Word Embeddings\n",
    "    - TF-IDF\n",
    "    - Contextual Word Embeddings\n",
    "    - Synonym\n",
    "    - Antonym\n",
    "    - Random Word\n",
    "    - Split\n",
    "- Sentence Augmenter\n",
    "    - Contextual Word Embeddings for Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"MODEL_DIR\"] = '../model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nlpaug\n",
    "# import nlpaug.augmenter.char as nac\n",
    "# import nlpaug.augmenter.word as naw\n",
    "# import nlpaug.augmenter.sentence as nas\n",
    "# import nlpaug.flow as nafc\n",
    "\n",
    "# from nlpaug.util import Action\n",
    "# nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = 'To improve my practical skills and allow me to conduct experiments independently.'\n",
    "# print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character Augmenter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Augmenting data in character level. Possible scenarios include image to text and chatbot. During recognizing text from image, we need to optical character recognition (OCR) model to achieve it but OCR introduces some errors such as recognizing \"o\" and \"0\". OCRAug simulate these errors to perform the data augmentation. For chatbot, we still have typo even though most of application comes with word correction. Therefore, KeyboardAug is introduced to similar this kind of errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OCR Augmenter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Substitute character by pre-defined OCR error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aug = nac.OcrAug()\n",
    "# augmented_texts = aug.augment(text, n=3)\n",
    "# print(\"Original:\")\n",
    "# print(text)\n",
    "# print(\"Augmented Texts:\")\n",
    "# print(augmented_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keyboard Augmenter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Substitute character by keyboard distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aug = nac.KeyboardAug()\n",
    "# augmented_text = aug.augment(text)\n",
    "# print(\"Original:\")\n",
    "# print(text)\n",
    "# print(\"Augmented Text:\")\n",
    "# print(augmented_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Augmenter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert character randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aug = nac.RandomCharAug(action=\"insert\")\n",
    "# augmented_text = aug.augment(text)\n",
    "# print(\"Original:\")\n",
    "# print(text)\n",
    "# print(\"Augmented Text:\")\n",
    "# print(augmented_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Substitute character randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aug = nac.RandomCharAug(action=\"substitute\")\n",
    "# augmented_text = aug.augment(text)\n",
    "# print(\"Original:\")\n",
    "# print(text)\n",
    "# print(\"Augmented Text:\")\n",
    "# print(augmented_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Swap character randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aug = nac.RandomCharAug(action=\"swap\")\n",
    "# augmented_text = aug.augment(text)\n",
    "# print(\"Original:\")\n",
    "# print(text)\n",
    "# print(\"Augmented Text:\")\n",
    "# print(augmented_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete character randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aug = nac.RandomCharAug(action=\"delete\")\n",
    "# augmented_text = aug.augment(text)\n",
    "# print(\"Original:\")\n",
    "# print(text)\n",
    "# print(\"Augmented Text:\")\n",
    "# print(augmented_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Augmenter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides character augmentation, word level is important as well. We make use of word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), fasttext (Joulin et al., 2016), BERT(Devlin et al., 2018) and wordnet to insert and substitute similar word. Word2vecAug, GloVeAug and FasttextAug use word embeddings to find most similar group of words to replace original word. On the other hand, BertAug use language models to predict possible target word. WordNetAug use statistics way to find the similar group of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spelling Augmenter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Substitute word by spelling mistake words dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aug = naw.SpellingAug(os.environ[\"MODEL_DIR\"] + 'spelling_en.txt')\n",
    "# augmented_texts = aug.augment(text, n=3)\n",
    "# print(\"Original:\")\n",
    "# print(text)\n",
    "# print(\"Augmented Texts:\")\n",
    "# print(augmented_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings Augmenter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert word randomly by word embeddings similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model_type: word2vec, glove or fasttext\n",
    "# aug = naw.WordEmbsAug(\n",
    "#     model_type='word2vec', model_path=model_dir+'GoogleNews-vectors-negative300.bin',\n",
    "#     action=\"insert\")\n",
    "# augmented_text = aug.augment(text)\n",
    "# print(\"Original:\")\n",
    "# print(text)\n",
    "# print(\"Augmented Text:\")\n",
    "# print(augmented_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Substitute word by word2vec similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model_type: word2vec, glove or fasttext\n",
    "# aug = naw.WordEmbsAug(\n",
    "#     model_type='word2vec', model_path=model_dir+'GoogleNews-vectors-negative300.bin',\n",
    "#     action=\"substitute\")\n",
    "# augmented_text = aug.augment(text)\n",
    "# print(\"Original:\")\n",
    "# print(text)\n",
    "# print(\"Augmented Text:\")\n",
    "# print(augmented_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Augmenter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert word by TF-IDF similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aug = naw.TfIdfAug(\n",
    "#     model_path=os.environ.get(\"MODEL_DIR\"),\n",
    "#     action=\"insert\")\n",
    "# augmented_text = aug.augment(text)\n",
    "# print(\"Original:\")\n",
    "# print(text)\n",
    "# print(\"Augmented Text:\")\n",
    "# print(augmented_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Substitute word by TF-IDF similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aug = naw.TfIdfAug(\n",
    "#     model_path=os.environ.get(\"MODEL_DIR\"),\n",
    "#     action=\"substitute\")\n",
    "# augmented_text = aug.augment(text)\n",
    "# print(\"Original:\")\n",
    "# print(text)\n",
    "# print(\"Augmented Text:\")\n",
    "# print(augmented_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contextual Word Embeddings Augmenter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert word by contextual word embeddings (BERT, DistilBERT, RoBERTA or XLNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aug = naw.ContextualWordEmbsAug(\n",
    "#     model_path='bert-base-uncased', action=\"insert\")\n",
    "# augmented_text = aug.augment(text)\n",
    "# print(\"Original:\")\n",
    "# print(text)\n",
    "# print(\"Augmented Text:\")\n",
    "# print(augmented_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Substitute word by contextual word embeddings (BERT, DistilBERT, RoBERTA or XLNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aug = naw.ContextualWordEmbsAug(\n",
    "#     model_path='bert-base-uncased', action=\"substitute\")\n",
    "# augmented_text = aug.augment(text)\n",
    "# print(\"Original:\")\n",
    "# print(text)\n",
    "# print(\"Augmented Text:\")\n",
    "# print(augmented_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aug = naw.ContextualWordEmbsAug(\n",
    "#     model_path='distilbert-base-uncased', action=\"substitute\")\n",
    "# augmented_text = aug.augment(text)\n",
    "# print(\"Original:\")\n",
    "# print(text)\n",
    "# print(\"Augmented Text:\")\n",
    "# print(augmented_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aug = naw.ContextualWordEmbsAug(\n",
    "#     model_path='roberta-base', action=\"substitute\")\n",
    "# augmented_text = aug.augment(text)\n",
    "# print(\"Original:\")\n",
    "# print(text)\n",
    "# print(\"Augmented Text:\")\n",
    "# print(augmented_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synonym Augmenter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Substitute word by WordNet's synonym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aug = naw.SynonymAug(aug_src='wordnet')\n",
    "# augmented_text = aug.augment(text)\n",
    "# print(\"Original:\")\n",
    "# print(text)\n",
    "# print(\"Augmented Text:\")\n",
    "# print(augmented_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Substitute word by PPDB's synonym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aug = naw.SynonymAug(aug_src='ppdb', model_path=os.environ.get(\"MODEL_DIR\") + 'ppdb-2.0-s-all')\n",
    "# augmented_text = aug.augment(text)\n",
    "# print(\"Original:\")\n",
    "# print(text)\n",
    "# print(\"Augmented Text:\")\n",
    "# print(augmented_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Antonym Augmenter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Substitute word by antonym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aug = naw.AntonymAug()\n",
    "# _text = 'Good boy'\n",
    "# augmented_text = aug.augment(_text)\n",
    "# print(\"Original:\")\n",
    "# print(_text)\n",
    "# print(\"Augmented Text:\")\n",
    "# print(augmented_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Word Augmenter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Swap word randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aug = nac.RandomWordAug(action=\"swap\")\n",
    "# augmented_text = aug.augment(text)\n",
    "# print(\"Original:\")\n",
    "# print(text)\n",
    "# print(\"Augmented Text:\")\n",
    "# print(augmented_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete word randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aug = naw.RandomWordAug()\n",
    "# augmented_text = aug.augment(text)\n",
    "# print(\"Original:\")\n",
    "# print(text)\n",
    "# print(\"Augmented Text:\")\n",
    "# print(augmented_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Augmenter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split word to two tokens randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aug = naw.SplitAug()\n",
    "# augmented_text = aug.augment(text)\n",
    "# print(\"Original:\")\n",
    "# print(text)\n",
    "# print(\"Augmented Text:\")\n",
    "# print(augmented_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contextual Word Embeddings for Sentence Augmenter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert sentence by contextual word embeddings (GPT2 or XLNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model_path: xlnet-base-cased or gpt2\n",
    "# aug = nas.ContextualWordEmbsForSentenceAug(model_path='xlnet-base-cased')\n",
    "# augmented_texts = aug.augment(text, n=3)\n",
    "# print(\"Original:\")\n",
    "# print(text)\n",
    "# print(\"Augmented Texts:\")\n",
    "# print(augmented_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aug = nas.ContextualWordEmbsForSentenceAug(model_path='gpt2')\n",
    "# augmented_text = aug.augment(text)\n",
    "# print(\"Original:\")\n",
    "# print(text)\n",
    "# print(\"Augmented Text:\")\n",
    "# print(augmented_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aug = nas.ContextualWordEmbsForSentenceAug(model_path='gpt2')\n",
    "# augmented_text = aug.augment(text)\n",
    "# print(\"Original:\")\n",
    "# print(text)\n",
    "# print(\"Augmented Text:\")\n",
    "# print(augmented_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aug = nas.ContextualWordEmbsForSentenceAug(model_path='distilgpt2')\n",
    "# augmented_text = aug.augment(text)\n",
    "# print(\"Original:\")\n",
    "# print(text)\n",
    "# print(\"Augmented Text:\")\n",
    "# print(augmented_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorisation of Q4 responses (x_train, x_test) <a name=\"vectorise\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorise(response):\n",
    "    '''Converts a text data file, post cleaning, into a matrix/vector form.\n",
    "    Input: Raw or clean data set (no stop works, word stems, meaningful words only)\n",
    "    \n",
    "\n",
    "    1) Calculates the max number of words in a response, then pads them with 0's to make the lengths of responses the same.\n",
    "    2) Assigns an integer value to each word in a response, based on its frequency. Lower frequency=higher integer.\n",
    "    \n",
    "    Output: A matrix with all responses as number elements, each column represents a response. '''\n",
    "\n",
    "    tokenizer= Tokenizer(oov_token=True)\n",
    "    tokenizer.fit_on_texts(response)\n",
    "\n",
    "    texts_numeric= tokenizer.texts_to_sequences(response)\n",
    "\n",
    "    # finding max length of reponses:\n",
    "    \n",
    "    len_texts_numeric = list(map(len, texts_numeric))\n",
    "    max_position = len_texts_numeric.index(max(len_texts_numeric))\n",
    "    my_item = texts_numeric[max_position]\n",
    "\n",
    "    response_max=len(texts_numeric[max_position])\n",
    "    print('The length of the longest response is:', response_max, ', Its position is:', max_position)\n",
    "\n",
    "    texts_pad = pad_sequences(texts_numeric, response_max) \n",
    "    \n",
    "    #response_max-this is the max number of words per response-->sets number of words for all responses to this\n",
    "    #max_position - the element number of the longest response\n",
    "    \n",
    "    return texts_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'aug2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-22b6c0b745ff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mxQ4vec_raw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maug2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mxQ4vec_clean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maug1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'aug2' is not defined"
     ]
    }
   ],
   "source": [
    "xQ4vec_raw = vectorise(aug2)\n",
    "xQ4vec_clean = vectorise(aug1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding maximum number of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw data\n",
    "a = []\n",
    "for i,x in enumerate(aug2):\n",
    "    a.append(x)\n",
    "\n",
    "max_words_raw = len(''.join(a))\n",
    "\n",
    "# clean data\n",
    "a = []\n",
    "for i,x in enumerate(aug1):\n",
    "    a.append(x)\n",
    "\n",
    "max_words_clean = len(''.join(a))\n",
    "\n",
    "print(max_words_raw)\n",
    "print(max_words_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot Encode With Multiple Labels - creating y_train and y_test <a name=\"onehot\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot = MultiLabelBinarizer()\n",
    "cat1 = np.array(pd.Series(dfq4_all.category1)).tolist()\n",
    "cat2 = np.array(pd.Series(dfq4_all.category2)).tolist()\n",
    "cat3 = np.array(pd.Series(dfq4_all.category3)).tolist()\n",
    "cat4 = np.array(pd.Series(dfq4_all.category4)).tolist()\n",
    "\n",
    "yQ4 = np.column_stack((cat1,cat2,cat3,cat4))\n",
    "yQ4vec = onehot.fit_transform(yQ4)\n",
    "\n",
    "print('Each column represents the classes:',onehot.classes_)\n",
    "print('The shape of the y vectors (corresponding to categories is',yQ4vec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment** = A value of 1 in the the rows for a particular columns means that the vectorised response belongs to that category. The categories are mutually exclusive; one y vector may have multiple 1s.\n",
    "\n",
    "**Comment** = NaN is a bit of an issue. As the responses in the training data do not have equal number of corresponding categories, NaN values are in place at the cells without an entry. Leaving this as an extra column in the one-hot encoded vectors is fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Model <a name=\"model\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NNmodel(x,y,x_words,embed_dim,epochs,batch_size):\n",
    "    '''Neural Network model for training \n",
    "       using train and test data\n",
    "       Inputs:  x, vectorised x data (responses)\n",
    "                y, vectorised y data (categories)\n",
    "                x_words, non-vectorised x as a list of str\n",
    "                embed_dim, dimension of embedding in embedding layer\n",
    "                epoch, epochs in model.fit\n",
    "       Outputs: model, model summary and training run\n",
    "                accr, accuracy and loss of the model\n",
    "                xtest, the test data\n",
    "                ytest, the categories of the test data'''\n",
    "    \n",
    "    ## Split data into training and test data ##\n",
    "    xtrain, xtest, ytrain, ytest = train_test_split(x,y,test_size=0.10,random_state=42)\n",
    "    \n",
    "    ## Number of distinct categories ##\n",
    "    y_no = len(y[1]) \n",
    "   \n",
    "    ## Finding maximum no of words ##\n",
    "    a = []\n",
    "    for i,x in enumerate(x_words):\n",
    "        a.append(x)\n",
    "    max_nb_words = len(''.join(a)) + 1      #should be 1 extra at least\n",
    "    \n",
    "    ## RNN Model ##\n",
    "    model = Sequential(name = \"RNN_model\")\n",
    "    model.add(Embedding(max_nb_words, embed_dim))\n",
    "    model.add(LSTM(150, dropout=0.2, recurrent_dropout=0.2))    # 150 LSTM cells\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(y_no, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    model.summary() # summary showing parameters\n",
    "    model.fit(xtrain, ytrain, epochs=epochs, batch_size=batch_size, validation_split=0.1) # fit data to model\n",
    "    \n",
    "    accr = model.evaluate(xtest,ytest) # accuracy\n",
    "\n",
    "    return model, accr, xtest, ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# onehot vectors with NaN category\n",
    "yQ4vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NaN category removed\n",
    "yQ4vec[:,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "x = xQ4vec_clean\n",
    "y = yQ4vec[:,1:] # NaN category removed\n",
    "x_words = aug1\n",
    "embed_dim = 1000\n",
    "epochs = 10\n",
    "batch_size = 50\n",
    "\n",
    "model, accr, xtest, ytest = NNmodel(x,y,x_words,embed_dim,epochs,batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot of model structure\n",
    "\n",
    "The following function is imported:\n",
    "\n",
    "**from keras.utils import plot_model**\n",
    "\n",
    "It requires both *pydot* and *GraphViz* to run. These need to be present on the computer. (If code does not work properly, it can be commented out.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model,show_shapes = True, show_layer_names = True, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loss and Accuracy\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment:** Accuracy increases and loss decreases with increasing number of epochs. 30 epochs gives quite a high accuracy in a reasonable amount of time. The accuracy hits 1.0 at around 50 epochs but takes a little time to get there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predictions using model\n",
    "# running on all the x data(both test and train data)\n",
    "print(model.predict(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy analysis <a name=\"accuracyanalysis\"></a>\n",
    "\n",
    "If the output is sparse multi-label, meaning a few positive labels and a majority are negative labels, the Keras accuracy metric will be overflatted by the correctly predicted negative labels. So the prediction would be [0, 0, 0, 0, 0, 1]. And if the actual labels were [0, 0, 0, 0, 0, 0], the accuracy would be 5/6 rather than 0 as it should be\n",
    "[1]. \n",
    "\n",
    "To get around this,a _custom accuracy_ rather than _accuracy_ can be used instead. The absolute accuracy is given by:\n",
    "\n",
    "$$\\text{Accuracy} = \\frac{\\text{No. data instances that are correctly classified}}{\\text{total number of data instances}}$$\n",
    "\n",
    "For example, if y_true is [1, 2, 3, 4] and y_pred is [0, 2, 3, 4] then the accuracy is 3/4 or .75. If the weights were specified as [1, 1, 0, 0] then the accuracy would be 1/2 or .5.\n",
    "\n",
    "This metric creates two local variables, total and count that are used to compute the frequency with which y_pred matches y_true. This frequency is ultimately returned as binary accuracy: an idempotent operation that simply divides total by count. [2]\n",
    "\n",
    "**Cateogrical accuracy** This metric is using the K.argmax method to compare the index of the maximal true value with the index of the maximal predicted value. In other words \"how often predictions have maximum in the same spot as true values\". For example, if y_true is [[0, 0, 1], [0, 1, 0]] and y_pred is [[0.1, 0.9, 0.8], [0.05, 0.95, 0]] then the categorical accuracy is 1/2 or .5 - One hot vectors. [5]\n",
    "\n",
    "K.mean(K.equal(K.argmax(y_true, axis=-1), K.argmax(y_pred, axis=-1)))\n",
    "axis=-1 means the axis along each element in the vector/matrix.\n",
    "\n",
    "k.argmax takes the highest value element of the prediction and compares it to the position of the highest elemental value in the actual set of labels.\n",
    "\n",
    "This means \"how often predictions have maximum in the same spot as true values\" [7]\n",
    "\n",
    "**top_k catgeroical accuracy**: Top-k categorical accuracy is almost similar to categorical accuracy. Here we calculate how often target class is within the top-k predictions. - One hot vectors. E.g. how often a class is in the top k index values. [4] \n",
    "\n",
    "**binary accuracy**: For example, if y_true is [1, 1, 0, 0] and y_pred is [0.98, 1, 0, 0.6] then the binary accuracy is 3/4 or .75. If the weights were specified as [1, 0, 0, 1] then the binary accuracy would be 1/2 or .5.\n",
    "\n",
    "This metric creates two local variables, total and count that are used to compute the frequency with which y_pred matches y_true. This frequency is ultimately returned as binary accuracy: an idempotent operation that simply divides total by count. [6]\n",
    "\n",
    "**loss - binary crossentropy**:\n",
    "Binary crossentropy is a loss function used on problems involving yes/no (binary) decisions. For instance, in multi-label problems, where an example can belong to multiple classes at the same time, the model tries to decide for each class whether the example belongs to that class or not. Model the output of the network as a independent Bernoulli distributions per label.[3]\n",
    "\n",
    "**Custom metric:**\n",
    "\n",
    "Similar to categorical accuracy but applied to multi hot vectors.\n",
    "\n",
    "OR\n",
    "\n",
    "**Hamming loss**\n",
    "THe fraction of wrongly predicted labels to the total number of labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision | Recall | F1-measure for different thresholds <a name=\"f1\"></a>\n",
    "From sklearn, three accuracy assessment functions are imported. The xtest and ytest arrays are run on the model and passes through these functions to test their accuracy. \n",
    "\n",
    "**precision_score**: The precision is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative. [8]\n",
    "\n",
    "$$\\frac{\\text{true positive}}{\\text{true positive + false positive}}$$\n",
    "\n",
    "**The best value is 1 and the worst value is 0.**\n",
    "\n",
    "**recall_score**: The recall is the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples. [9]\n",
    "\n",
    "$$\\frac{\\text{true positive}}{\\text{true positive + false negative}}$$\n",
    "\n",
    "**The best value is 1 and the worst value is 0.**\n",
    "\n",
    "**f1_score**: The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal. The formula for the F1 score is: [10]\n",
    "\n",
    "$$ F1 = \\frac{2 * \\text{precision} * \\text{recall} }{ \\text{precision} + \\text{recall}} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[11]\n",
    "predictions=model.predict([xtest])\n",
    "\n",
    "thresholds=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "for val in thresholds:\n",
    "    pred=predictions.copy()\n",
    "  \n",
    "    pred[pred>=val]=1\n",
    "    pred[pred<val]=0\n",
    "  \n",
    "    precision = precision_score(ytest, pred, average='micro')\n",
    "    recall = recall_score(ytest, pred, average='micro')\n",
    "    f1 = f1_score(ytest, pred, average='micro')\n",
    "   \n",
    "    print(\"Micro-average quality numbers\")\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on unseen data <a name=\"unseen\"></a>\n",
    "We will now run our model on the questionnaire data from the academic year 2019-2020 to automatically classify the responses. First, we import the Q4 data. This data is not distinguished between native and non-native English speakers because the questionnaire question had been changed to what qualifications they had prior to university, i.e. whether they had done A-Levels, as students educational background relate more to their expectations than whether their first language is English. A lot of overseas students do A levels without their first language being English. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Q4 data 2019-2020\n",
    "q4_1920 = import_df('Q4_1920.csv')\n",
    "### Dataset ###\n",
    "dfq4_1920 = pd.DataFrame()\n",
    "dfq4_1920 = q4_1920[1:]\n",
    "\n",
    "dfq4_1920 = dfq4_1920.rename(columns={0:\"response\"})\n",
    "\n",
    "#print\n",
    "#dfq4_1920.response \n",
    "\n",
    "q4_1920_clean = clean_text(dfq4_1920.response.tolist())\n",
    "\n",
    "xQ4_1920_vec = vectorise(q4_1920_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4_cat_vecs = model.predict(xQ4_1920_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for precision, recall and f1score <a name=\"accuracygraph\"></a>\n",
    "#### Date: 07/03/2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_plot(xtest,ytest):\n",
    "    '''Outputs the precision, recall and f1score of the\n",
    "       outcome of the test data when they are tested \n",
    "       using the model\n",
    "       Inputs:  xtest, test data of responses\n",
    "                ytest, test data of categories\n",
    "       Output:  plot of scores against threshold'''\n",
    "    \n",
    "    predictions=model.predict([xtest])\n",
    "\n",
    "    thresholds=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "  \n",
    "    p = np.zeros(len(thresholds))\n",
    "    r = np.zeros_like(p)\n",
    "    f = np.zeros_like(p)\n",
    "    \n",
    "    for i,val in enumerate(thresholds):\n",
    "        pred=predictions.copy()\n",
    "  \n",
    "        pred[pred>=val]=1\n",
    "        pred[pred<val]=0\n",
    "  \n",
    "        precision = precision_score(ytest, pred, average='micro')\n",
    "        p[i] = precision\n",
    "        recall = recall_score(ytest, pred, average='micro')\n",
    "        r[i] = recall\n",
    "        f1 = f1_score(ytest, pred, average='micro')\n",
    "        f[i] = f1\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(thresholds,p,label='Precision')\n",
    "    plt.plot(thresholds,r,label='Recall')\n",
    "    plt.plot(thresholds,f,label='F1')\n",
    "    plt.xlabel('Threshold for accepting column')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend(loc='best') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_plot(xtest,ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for threshold and outputting categories <a name=\"categoryoutput\"></a>\n",
    "#### Date: 07/03/2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_output(yvecs, threshold):\n",
    "    '''Takes the resulting multi one-hot encoded vectors and \n",
    "       returns corresponding categories for a specified threshold\n",
    "       Inputs: yvecs, category vector predictions of the model\n",
    "               threshold, thresholds for accepting columns\n",
    "       Output: list of \n",
    "    '''\n",
    "    onehotvecs = onehot.classes_[1:]\n",
    "    \n",
    "    cat = np.zeros((yvecs.shape),dtype='<U2')\n",
    "    for index,val in np.ndenumerate(yvecs):\n",
    "        if val >= threshold:\n",
    "            category = onehotvecs[index[1:]]\n",
    "            cat[index] = category\n",
    "    \n",
    "    return cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing with different parameters, thresholds and optimising <a name=\"optimise\"></a>\n",
    "### Effect of adding Gaussian Noise - RUN 1 <a name=\"run1\"></a>\n",
    "\n",
    "We can see that the accuracy of the training dataset is much better than that of the test, a possible sign of overfitting.\n",
    "\n",
    "Noise is used as a regulariazation method to prevent overfitting and has the effect of creating more samples or resampling the domain. It is useful for small training data sets.\n",
    "\n",
    "It can be used to add noise to the input data as an input layer in the model. It can also be added inbetween layers.\n",
    "\n",
    "The output of the noise layer will have the same shape as the input, with the only modification being the addition of noise to the values. As the noise is Gaussian, its mean will be zero and will require a standard deviation input. \n",
    "\n",
    "**Variables and Parameters:**\n",
    "- x = xQ4vec_clean\n",
    "- y = yQ4vec[:,1:] \n",
    "- x_words = q4response_clean\n",
    "- embed_dim = 1000\n",
    "- epochs = 10\n",
    "- batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## new model with noise\n",
    "def NNmodelNoise(x,y,x_words,embed_dim,epochs,batch_size, max_words):\n",
    "    \n",
    "    ## Split data into training and test data ##\n",
    "    xtrain, xtest, ytrain, ytest = train_test_split(x,y,test_size=0.10,random_state=42)\n",
    "    \n",
    "    ## Number of distinct categories ##\n",
    "    y_no = len(y[1]) \n",
    "   \n",
    "    ## Finding maximum no of words ##\n",
    "    a = []\n",
    "    for i,x in enumerate(x_words):\n",
    "        a.append(x)\n",
    "    max_nb_words = len(''.join(a)) + 1      #should be 1 extra at least\n",
    "    \n",
    "    ## RNN Model ##\n",
    "    model = Sequential(name = \"RNN_model\")\n",
    "    model.add(Embedding(max_nb_words, embed_dim))\n",
    "    #adding Gaussian Noise   [10]\n",
    "    model.add(GaussianNoise(0.001, input_shape=(max_words,100)))\n",
    "    model.add(LSTM(150, dropout=0.2, recurrent_dropout=0.2))    # 150 LSTM cells\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(y_no, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    model.summary() # summary showing parameters\n",
    "    model.fit(xtrain, ytrain, epochs=epochs, batch_size=batch_size, validation_split=0.1) # fit data to model\n",
    "    \n",
    "    accr = model.evaluate(xtest,ytest) # accuracy\n",
    "\n",
    "    return model, accr, xtest, ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = xQ4vec_clean\n",
    "y = yQ4vec[:,1:] # NaN category removed\n",
    "x_words = q4response_clean\n",
    "embed_dim = 1000\n",
    "epochs = 10\n",
    "batch_size = 50\n",
    "max_words = 27 # Gaussian noise parameter\n",
    "\n",
    "model, accrNoise, xtestNoise, ytestNoise = NNmodelNoise(x,y,x_words,embed_dim,epochs,batch_size, max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accRun1 = accuracy_plot(xtest, ytest)\n",
    "plt.title('Effect of adding Gaussian noise (LSTM = 150)')\n",
    "plt.savefig('run1.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the Gaussian noise improves recall and the overall F1.\n",
    "\n",
    "### Adding Batch Normalisation - RUN 2 <a name=\"run2\"></a>\n",
    "**BatchNormalization**: normalizes outputs independently to the weight updates of the previous layer\n",
    "**Variables and Parameters**:\n",
    "- x = xQ4vec_clean\n",
    "- y = yQ4vec[:,1:] \n",
    "- x_words = q4response_clean\n",
    "- embed_dim = 1000\n",
    "- epochs = 10\n",
    "- batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NNmodelNorm(x,y,x_words,embed_dim,epochs,batch_size):\n",
    "    \n",
    "    ## Split data into training and test data ##\n",
    "    xtrain, xtest, ytrain, ytest = train_test_split(x,y,test_size=0.10,random_state=42)\n",
    "    \n",
    "    ## Number of distinct categories ##\n",
    "    y_no = len(y[1]) \n",
    "   \n",
    "    ## Finding maximum no of words ##\n",
    "    a = []\n",
    "    for i,x in enumerate(x_words):\n",
    "        a.append(x)\n",
    "    max_nb_words = len(''.join(a)) + 1      #should be 1 extra at least\n",
    "    \n",
    "    ## RNN Model ##\n",
    "    model = Sequential(name = \"RNN_model\")\n",
    "    model.add(Embedding(max_nb_words, embed_dim))\n",
    "    model.add(LSTM(150, dropout=0.2, recurrent_dropout=0.2, kernel_initializer='normal')) # 150 LSTM cells\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(y_no, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    model.summary() # summary showing parameters\n",
    "    model.fit(xtrain, ytrain, epochs=epochs, batch_size=batch_size, validation_split=0.1) # fit data to model\n",
    "    \n",
    "    accr = model.evaluate(xtest,ytest) # accuracy\n",
    "\n",
    "    return model, accr, xtest, ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = xQ4vec_clean\n",
    "y = yQ4vec[:,1:] # NaN category removed\n",
    "x_words = q4response_clean\n",
    "embed_dim = 1000\n",
    "epochs = 10\n",
    "batch_size = 50\n",
    "\n",
    "model, accrNorm, xtestNorm, ytestNorm = NNmodelNorm(x,y,x_words,embed_dim,epochs,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accRun2 = accuracy_plot(xtestNorm, ytestNorm)\n",
    "plt.title('Effect of adding batch normalisation (LSTM = 150)')\n",
    "plt.savefig('run2.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Kernel Initializer - RUN 3 <a name=\"run3\"></a>\n",
    "**kernel-intializer**: initializes weights after each layer using a statistical distribution\n",
    "**Variables and Parameters**:\n",
    "- x = xQ4vec_clean\n",
    "- y = yQ4vec[:,1:] \n",
    "- x_words = q4response_clean\n",
    "- embed_dim = 1000\n",
    "- epochs = 10\n",
    "- batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NNmodelIni(x,y,x_words,embed_dim,epochs,batch_size):\n",
    "    \n",
    "    ## Split data into training and test data ##\n",
    "    xtrain, xtest, ytrain, ytest = train_test_split(x,y,test_size=0.10,random_state=42)\n",
    "    \n",
    "    ## Number of distinct categories ##\n",
    "    y_no = len(y[1]) \n",
    "   \n",
    "    ## Finding maximum no of words ##\n",
    "    a = []\n",
    "    for i,x in enumerate(x_words):\n",
    "        a.append(x)\n",
    "    max_nb_words = len(''.join(a)) + 1      #should be 1 extra at least\n",
    "    \n",
    "    ## RNN Model ##\n",
    "    model = Sequential(name = \"RNN_model\")\n",
    "    model.add(Embedding(max_nb_words, embed_dim))\n",
    "    model.add(LSTM(150, dropout=0.2, recurrent_dropout=0.2, kernel_initializer='normal')) # 150 LSTM cells\n",
    "    model.add(Dense(y_no, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    model.summary() # summary showing parameters\n",
    "    model.fit(xtrain, ytrain, epochs=epochs, batch_size=batch_size, validation_split=0.1) # fit data to model\n",
    "    \n",
    "    accr = model.evaluate(xtest,ytest) # accuracy\n",
    "\n",
    "    return model, accr, xtest, ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = xQ4vec_clean\n",
    "y = yQ4vec[:,1:] # NaN category removed\n",
    "x_words = q4response_clean\n",
    "embed_dim = 1000\n",
    "epochs = 10\n",
    "batch_size = 50\n",
    "\n",
    "model, accrIni, xtestIni, ytestIni = NNmodelIni(x,y,x_words,embed_dim,epochs,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accRun3 = accuracy_plot(xtestIni, ytestIni)\n",
    "plt.title('Effect of adding Kernel initializer (LSTM = 150)')\n",
    "plt.savefig('run3.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly the batch normalization hasn't improved F1 accuracy...But the kernel initializer has.\n",
    "\n",
    "### Noise + Normalisation + Kernal Initializer - RUN 4 <a name=\"run4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NNmodelAll(x,y,x_words,embed_dim,epochs,batch_size):\n",
    "    \n",
    "    ## Split data into training and test data ##\n",
    "    xtrain, xtest, ytrain, ytest = train_test_split(x,y,test_size=0.10,random_state=42)\n",
    "    \n",
    "    ## Number of distinct categories ##\n",
    "    y_no = len(y[1]) \n",
    "   \n",
    "    ## Finding maximum no of words ##\n",
    "    a = []\n",
    "    for i,x in enumerate(x_words):\n",
    "        a.append(x)\n",
    "    max_nb_words = len(''.join(a)) + 1      #should be 1 extra at least\n",
    "    \n",
    "    ## RNN Model ##\n",
    "    model = Sequential(name = \"RNN_model\")\n",
    "    model.add(Embedding(max_nb_words, embed_dim))\n",
    "    #adding Gaussian Noise   [10]\n",
    "    model.add(GaussianNoise(0.001, input_shape=(27,100)))\n",
    "    model.add(LSTM(150, dropout=0.2, recurrent_dropout=0.2, kernel_initializer='normal'))    # 150 LSTM cells\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(y_no, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    model.summary() # summary showing parameters\n",
    "    model.fit(xtrain, ytrain, epochs=epochs, batch_size=batch_size, validation_split=0.1) # fit data to model\n",
    "    \n",
    "    accr = model.evaluate(xtest,ytest) # accuracy\n",
    "\n",
    "    return model, accr, xtest, ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = xQ4vec_clean\n",
    "y = yQ4vec[:,1:] # NaN category removed\n",
    "x_words = q4response_clean\n",
    "embed_dim = 1000\n",
    "epochs = 10\n",
    "batch_size = 50\n",
    "\n",
    "model, accrAll, xtestAll, ytestAll = NNmodelAll(x,y,x_words,embed_dim,epochs,batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accRun4 = accuracy_plot(xtestAll, ytestAll)\n",
    "plt.title('Gaussian noise & Batch Normalisation & Kernel Initializer (LSTM = 150)')\n",
    "plt.savefig('run4.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalisation + Initializer - RUN 5 <a name=\"run5\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NNmodelNI(x,y,x_words,embed_dim,epochs,batch_size):\n",
    "\n",
    "    ## Split data into training and test data ##\n",
    "    xtrain, xtest, ytrain, ytest = train_test_split(x,y,test_size=0.10,random_state=42)\n",
    "    \n",
    "    ## Number of distinct categories ##\n",
    "    y_no = len(y[1]) \n",
    "   \n",
    "    ## Finding maximum no of words ##\n",
    "    a = []\n",
    "    for i,x in enumerate(x_words):\n",
    "        a.append(x)\n",
    "    max_nb_words = len(''.join(a)) + 1      #should be 1 extra at least\n",
    "    \n",
    "    ## RNN Model ##\n",
    "    model = Sequential(name = \"RNN_model\")\n",
    "    model.add(Embedding(max_nb_words, embed_dim))\n",
    "    #adding Gaussian Noise   [10]\n",
    "    model.add(GaussianNoise(0.001, input_shape=(27,100)))\n",
    "    model.add(LSTM(150, dropout=0.2, recurrent_dropout=0.2, kernel_initializer='normal'))    # 150 LSTM cells\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(y_no, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    model.summary() # summary showing parameters\n",
    "    model.fit(xtrain, ytrain, epochs=epochs, batch_size=batch_size, validation_split=0.1) # fit data to model\n",
    "    \n",
    "    accr = model.evaluate(xtest,ytest) # accuracy\n",
    "\n",
    "    return model, accr, xtest, ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = xQ4vec_clean\n",
    "y = yQ4vec[:,1:] # NaN category removed\n",
    "x_words = q4response_clean\n",
    "embed_dim = 1000\n",
    "epochs = 10\n",
    "batch_size = 50\n",
    "\n",
    "model, accrNI, xtestNI, ytestNI = NNmodelNI(x,y,x_words,embed_dim,epochs,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accRun5 = accuracy_plot(xtestNI, ytestNI)\n",
    "plt.title('Batch normalisation & Kernel initializer (LSTM = 150)')\n",
    "plt.savefig('run5.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Function - Random Search/Grid Search to find best parameters <a name=\"run6\"></a>\n",
    "This allows us to randomly try different parameter values such as those below, to see which gives us the best accuracy. This uses the Skikit learn estimator.\n",
    "\n",
    "There are many different ways to do this...\n",
    "\n",
    "- **Grid search**: searches all the possible configurations of the different parameters in order to find the best one.\n",
    "- **Random search**: similar to Grid search but picks the point out randomly from the configuration space. This means that the hyperparameter space has been searched through more widely; allowing us to find the best configuration in less iterations.\n",
    "\n",
    "We first want to create a function that describes our model. Within this we have added a _for loop_ to allow us to see how many layers and how many neurons per layer, is optimal when we later apply the _random search_. \n",
    "\n",
    "We will then convert the model to a sklearn estimator that will then run all the parameter configurations through the model. We must use a low number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split data into training and test data ##\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x,y,test_size=0.10,random_state=42)\n",
    "\n",
    "\n",
    "#nl: number of layers, nn: number of neurons\n",
    "def create_model(nl=1, nn=25, std=0.001, dropout=0.2, recurrent_dropout=0.2, neu=150):\n",
    "    \n",
    "    ## Number of distinct categories ##\n",
    "    y_no = len(y[1]) \n",
    "   \n",
    "    ## Finding maximum no of words ##\n",
    "    a = []\n",
    "    for i,x in enumerate(x_words):\n",
    "        a.append(x)\n",
    "    max_nb_words = len(''.join(a)) + 1      #should be 1 extra at least\n",
    "    \n",
    "    model=Sequential()\n",
    "    model.add(Embedding(max_nb_words, embed_dim))\n",
    "    \n",
    "    #adding Gaussian Noise   [10]\n",
    "    model.add(GaussianNoise(std, input_shape=(max_words,100)))\n",
    "    model.add(LSTM(neu, dropout=dropout, recurrent_dropout=recurrent_dropout))\n",
    "\n",
    "    for i in range(nl):\n",
    "        model.add(Dense(nn, activation='sigmoid'))\n",
    "        \n",
    "    model.add(Dense(y_no, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "#create a model as a sklearn estimator\n",
    "kerasmodel=KerasClassifier(build_fn=create_model, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a series of parameters\n",
    "\n",
    "#params=dict( batch_size=[80,100,120], nl=[0,1,2], nn=[100,50,25], std=[1,5])\n",
    "#params=dict(neu=[50,100,150,200,500], std=[0.00001, 0.0001, 0.01], batch_size=[40,60,80,100,120], nl=[0,1,2], nn=[100,150,25,50], dropout=[0.1,0.2,0.3,0.4])\n",
    "\n",
    "#random_search=RandomizedSearchCV(kerasmodel,param_distributions=params,cv=5)\n",
    "\n",
    "#print best results\n",
    "#random_search_results=random_search.fit(x,y)\n",
    "\n",
    "#results: best model parameters\n",
    "#print(\"Best: %f using %s\"%(random_search_results.best_score_, random_search_results.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Search suggests the best parameters are:\n",
    "\n",
    "- batch size: 60\n",
    "- dropout: 0.2\n",
    "- 1 dense layer\n",
    "- standard deviation of Gaussian noise: 0.00001\n",
    "- LSTM with 200 neurons\n",
    "\n",
    "out of the following parameters:\n",
    "params=dict(neu=[50,100,150,200,500], std=[0.00001, 0.0001, 0.01], batch_size=[40,60,80,100,120], nl=[0,1,2], nn=[100,150,25,50], dropout=[0.1,0.2,0.3,0.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NNmodelBest(x,y,x_words,embed_dim,epochs,batch_size,max_words):\n",
    "    '''Neural Network model for training \n",
    "       using train and test data\n",
    "       Inputs:  x, vectorised x data (responses)\n",
    "                y, vectorised y data (categories)\n",
    "                x_words, non-vectorised x as a list of str\n",
    "                embed_dim, dimension of embedding in embedding layer\n",
    "                epoch, epochs in model.fit\n",
    "       Outputs: '''\n",
    "    \n",
    "    ## Split data into training and test data ##\n",
    "    xtrain, xtest, ytrain, ytest = train_test_split(x,y,test_size=0.10,random_state=42)\n",
    "    \n",
    "    ## Number of distinct categories ##\n",
    "    y_no = len(y[1]) \n",
    "   \n",
    "    ## Finding maximum no of words ##\n",
    "    a = []\n",
    "    for i,x in enumerate(x_words):\n",
    "        a.append(x)\n",
    "    max_nb_words = len(''.join(a)) + 1      #should be 1 extra at least\n",
    "    \n",
    "    ## RNN Model ##\n",
    "    model = Sequential(name = \"RNN_model\")\n",
    "    model.add(Embedding(max_nb_words, embed_dim))\n",
    "    #adding Gaussian Noise   [10]\n",
    "    model.add(GaussianNoise(0.00001, input_shape=(max_words,100)))\n",
    "    model.add(LSTM(200, dropout=0.2, recurrent_dropout=0.2))    # 150 LSTM cells\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(y_no, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    model.summary() # summary showing parameters\n",
    "    model.fit(xtrain, ytrain, epochs=epochs, batch_size=batch_size, validation_split=0.1) # fit data to model\n",
    "    \n",
    "    accr = model.evaluate(xtest,ytest) # accuracy\n",
    "\n",
    "    return model, accr, xtest, ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = xQ4vec_clean\n",
    "y = yQ4vec[:,1:] # NaN category removed\n",
    "x_words = q4response_clean\n",
    "embed_dim = 1000\n",
    "epochs = 10\n",
    "batch_size = 60\n",
    "\n",
    "model, accrBest, xtestBest, ytestBest = NNmodelBest(x,y,x_words,embed_dim,epochs,batch_size, max_words=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accRun6 = accuracy_plot(xtestBest, ytestBest)\n",
    "plt.title('Batch_size = 60 & LSTM = 200')\n",
    "plt.savefig('run6.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigating the effect of changing LSTM cells and embed_dim <a name=\"run7\">\n",
    "\n",
    "### LSTM cells - Run 7 \n",
    "The number of LSTM cells increased from 150 to 300.\n",
    "\n",
    "**Variables and Parameters**:\n",
    "- x = xQ4vec_clean\n",
    "- y = yQ4vec[:,1:] \n",
    "- x_words = q4response_clean\n",
    "- embed_dim = 1000\n",
    "- epochs = 10\n",
    "- batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NNmodel_lstm(x,y,x_words,embed_dim,epochs,batch_size):\n",
    "\n",
    "    ## Split data into training and test data ##\n",
    "    xtrain, xtest, ytrain, ytest = train_test_split(x,y,test_size=0.10,random_state=42)\n",
    "    \n",
    "    ## Number of distinct categories ##\n",
    "    y_no = len(y[1]) \n",
    "   \n",
    "    ## Finding maximum no of words ##\n",
    "    a = []\n",
    "    for i,x in enumerate(x_words):\n",
    "        a.append(x)\n",
    "    max_nb_words = len(''.join(a)) + 1      #should be 1 extra at least\n",
    "    \n",
    "    ## RNN Model ##\n",
    "    model = Sequential(name = \"RNN_model\")\n",
    "    model.add(Embedding(max_nb_words, embed_dim))\n",
    "    #adding Gaussian Noise   [10]\n",
    "    model.add(GaussianNoise(0.001, input_shape=(27,100)))\n",
    "    model.add(LSTM(300, dropout=0.2, recurrent_dropout=0.2, kernel_initializer='normal'))    # 300 LSTM cells\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(y_no, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    model.summary() # summary showing parameters\n",
    "    model.fit(xtrain, ytrain, epochs=epochs, batch_size=batch_size, validation_split=0.1) # fit data to model\n",
    "    \n",
    "    accr = model.evaluate(xtest,ytest) # accuracy\n",
    "\n",
    "    return model, accr, xtest, ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = xQ4vec_clean\n",
    "y = yQ4vec[:,1:] # NaN category removed\n",
    "x_words = q4response_clean\n",
    "embed_dim = 1000\n",
    "epochs = 10\n",
    "batch_size = 50\n",
    "\n",
    "model, accrLSTM, xtestLSTM, ytestLSTM = NNmodel_lstm(x,y,x_words,embed_dim,epochs,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accRun7 = accuracy_plot(xtestLSTM,ytestLSTM)\n",
    "plt.title('Embed_dim = 1000 & LSTM = 300')\n",
    "plt.savefig('run7.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment:** The doubling of the number of LSTM cells increased the value of f1 score slighlty, and also made it more flat across the thresholds. It does not slow the code significantly, therefore 300 LSTM cells can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embed_dim - Run 8 <a name=\"run8\">\n",
    "**Variables and Parameters**:\n",
    "- x = xQ4vec_clean\n",
    "- y = yQ4vec[:,1:] \n",
    "- x_words = q4response_clean\n",
    "- embed_dim = 3000\n",
    "- epochs = 10\n",
    "- batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NNmodel_embeddim(x,y,x_words,embed_dim,epochs,batch_size):\n",
    "\n",
    "    ## Split data into training and test data ##\n",
    "    xtrain, xtest, ytrain, ytest = train_test_split(x,y,test_size=0.10,random_state=42)\n",
    "    \n",
    "    ## Number of distinct categories ##\n",
    "    y_no = len(y[1]) \n",
    "   \n",
    "    ## Finding maximum no of words ##\n",
    "    a = []\n",
    "    for i,x in enumerate(x_words):\n",
    "        a.append(x)\n",
    "    max_nb_words = len(''.join(a)) + 1      #should be 1 extra at least\n",
    "    \n",
    "    ## RNN Model ##\n",
    "    model = Sequential(name = \"RNN_model\")\n",
    "    model.add(Embedding(max_nb_words, embed_dim))\n",
    "    #adding Gaussian Noise   [10]\n",
    "    model.add(GaussianNoise(0.001, input_shape=(27,100)))\n",
    "    model.add(LSTM(300, dropout=0.2, recurrent_dropout=0.2, kernel_initializer='normal'))    # 300 LSTM cells\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(y_no, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    model.summary() # summary showing parameters\n",
    "    model.fit(xtrain, ytrain, epochs=epochs, batch_size=batch_size, validation_split=0.1) # fit data to model\n",
    "    \n",
    "    accr = model.evaluate(xtest,ytest) # accuracy\n",
    "\n",
    "    return model, accr, xtest, ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = xQ4vec_clean\n",
    "y = yQ4vec[:,1:] \n",
    "x_words = q4response_clean\n",
    "embed_dim = 3000\n",
    "epochs = 10\n",
    "batch_size = 50\n",
    "\n",
    "model, accrembed, xtestembed, ytestembed = NNmodel_embeddim(x,y,x_words,embed_dim,epochs,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accRun8 = accuracy_plot(xtestembed,ytestembed)\n",
    "plt.title('Embed_dim = 3000 & LSTM = 300')\n",
    "plt.savefig('run8.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing embedding dimension from 1000 to 3000 increased the f1 score which gave a peak at 0.3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPTIMISED MODEL - BATCH SIZE REDUCED <a name=\"finalmodel\">\n",
    "**Variables and Parameters:**  \n",
    "\n",
    "- x = xQ4vec_clean\n",
    "- y = yQ4vec[:,1:] \n",
    "- x_words = q4response_clean\n",
    "- embed_dim = 1500\n",
    "- epochs = 10\n",
    "- batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q4_categorise(x,y,x_words,embed_dim,epochs,batch_size):\n",
    "    '''Neural Network model for training \n",
    "       using train and test data\n",
    "       Inputs:  x, vectorised x data (responses)\n",
    "                y, vectorised y data (categories)\n",
    "                x_words, non-vectorised x as a list of str\n",
    "                embed_dim, dimension of embedding in embedding layer\n",
    "                epoch, epochs in model.fit\n",
    "       Outputs: model, model summary and training run\n",
    "                accr, accuracy and loss of the model\n",
    "                xtest, the test data\n",
    "                ytest, the categories of the test data'''\n",
    "    ## Split data into training and test data ##\n",
    "    xtrain, xtest, ytrain, ytest = train_test_split(x,y,test_size=0.10,random_state=42)\n",
    "    \n",
    "    ## Number of distinct categories ##\n",
    "    y_no = len(y[1]) \n",
    "   \n",
    "    ## Finding maximum no of words ##\n",
    "    a = []\n",
    "    for i,x in enumerate(x_words):\n",
    "        a.append(x)\n",
    "    max_nb_words = len(''.join(a)) + 1      #should be 1 extra at least\n",
    "    \n",
    "    ## RNN Model ##\n",
    "    model = Sequential(name = \"RNN_model\")\n",
    "    model.add(Embedding(max_nb_words, embed_dim))\n",
    "    #adding Gaussian Noise   [10]\n",
    "    model.add(GaussianNoise(0.001, input_shape=(27,100)))\n",
    "    model.add(LSTM(300, dropout=0.2, recurrent_dropout=0.2, kernel_initializer='normal'))    # 300 LSTM cells\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(y_no, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    model.summary() # summary showing parameters\n",
    "    model.fit(xtrain, ytrain, epochs=epochs, batch_size=batch_size, validation_split=0.1) # fit data to model\n",
    "    \n",
    "    accr = model.evaluate(xtest,ytest) # accuracy\n",
    "\n",
    "    return model, accr, xtest, ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = xQ4vec_clean\n",
    "y = yQ4vec[:,1:] \n",
    "x_words = q4response_clean\n",
    "embed_dim = 1500\n",
    "epochs = 10\n",
    "batch_size = 10\n",
    "\n",
    "model, accr, xtest, ytest = Q4_categorise(x,y,x_words,embed_dim,epochs,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model,show_shapes = True, show_layer_names = True, to_file='modelfinal.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = accuracy_plot(xtest,ytest)\n",
    "plt.title('Embed_dim = 1500 & Batch_size = 10 & LSTM = 300')\n",
    "plt.savefig('final.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Run on 2019 with category outputs<a name=\"final\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4_cat_vecs = model.predict(xQ4_1920_vec)\n",
    "#print(Q4_cat_vecs)\n",
    "categorized = cat_output(Q4_cat_vecs,threshold=0.4) # HS function used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(categorized[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = Q4_cat_vecs >= 0.4\n",
    "class_dict = {}\n",
    "for example in range(len(pred)):\n",
    "    for class_ in range(len(pred[0])):\n",
    "        if pred[example][class_] == True:\n",
    "            if example in class_dict:\n",
    "                class_dict[example].append(onehot.classes_[1:][class_])\n",
    "            else:\n",
    "                class_dict[example] = [onehot.classes_[1:][class_]]\n",
    "print(class_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categories: 'TD' 'TR' 'TT' 'TU' 'TX' 'UD' 'UI' 'UP' 'US' 'UT' 'UU' 'UX'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = len(categorized[0])\n",
    "rows = len(categorized)\n",
    "\n",
    "td = 0\n",
    "for j in range(columns):\n",
    "    for i in range(rows):\n",
    "        if categorized[i,j] == 'TD':\n",
    "            td = td + 1\n",
    "tr = 0\n",
    "for j in range(columns):\n",
    "    for i in range(rows):\n",
    "        if categorized[i,j] == 'TR':\n",
    "            tr = tr + 1\n",
    "tt = 0\n",
    "for j in range(columns):\n",
    "    for i in range(rows):\n",
    "        if categorized[i,j] == 'TT':\n",
    "            tt = tt + 1\n",
    "tu = 0\n",
    "for j in range(columns):\n",
    "    for i in range(rows):\n",
    "        if categorized[i,j] == 'TU':\n",
    "            tu = tu + 1\n",
    "tx = 0\n",
    "for j in range(columns):\n",
    "    for i in range(rows):\n",
    "        if categorized[i,j] == 'TX':\n",
    "            tx = tx + 1\n",
    "ud = 0\n",
    "for j in range(columns):\n",
    "    for i in range(rows):\n",
    "        if categorized[i,j] == 'UD':\n",
    "            ud = ud + 1\n",
    "ui = 0\n",
    "for j in range(columns):\n",
    "    for i in range(rows):\n",
    "        if categorized[i,j] == 'UI':\n",
    "            ui = ui + 1\n",
    "up = 0\n",
    "for j in range(columns):\n",
    "    for i in range(rows):\n",
    "        if categorized[i,j] == 'UP':\n",
    "            up = up + 1\n",
    "us = 0\n",
    "for j in range(columns):\n",
    "    for i in range(rows):\n",
    "        if categorized[i,j] == 'US':\n",
    "            us = us + 1\n",
    "ut = 0\n",
    "for j in range(columns):\n",
    "    for i in range(rows):\n",
    "        if categorized[i,j] == 'UT':\n",
    "            ut = ut + 1\n",
    "uu = 0\n",
    "for j in range(columns):\n",
    "    for i in range(rows):\n",
    "        if categorized[i,j] == 'UU':\n",
    "            uu = uu + 1\n",
    "ux = 0\n",
    "for j in range(columns):\n",
    "    for i in range(rows):\n",
    "        if categorized[i,j] == 'UX':\n",
    "            ux = ux + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot of frequencies of each category for 2019-2020\n",
    "y = [td,tr,tt,tu,tx,ud,ui,up,us,ut,uu,ux]\n",
    "y_arr = np.array(y)\n",
    "x_bars = ('TD','TR','TT','TU','TX','UD','UI','UP','US','UT','UU','UX')\n",
    "color = np.linspace(0,1,11)\n",
    "plt.bar(x_bars,y_arr,color = ['red','blue','brown','green','orange'])\n",
    "plt.title('Automatic categorisation of Q4 2019-2020 Data')\n",
    "plt.ylabel('Number of occurences')\n",
    "plt.savefig('q4automatic.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvement - Data Augmentation <a name=\"augmentation\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(Q4_categorise)\n",
    "\n",
    "# model, accr, xtest, ytest = Q4_categorise(x,y,x_words,embed_dim,epochs,batch_size)\n",
    "\n",
    "# m = Q4_categorise(x,y,x_words,embed_dim,epochs,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk import word_tokenize\n",
    "# import random\n",
    "\n",
    "# q4response_raw = dfq4_all.Response.tolist()\n",
    "# q4response_clean = clean_text(dfq4_all.Response.tolist())\n",
    "# aug = unaug = q4response_clean\n",
    "\n",
    "# def augment(sentence,n):\n",
    "#     new_sentences = []\n",
    "#     words = word_tokenize(sentence)\n",
    "#     for i in range(n):\n",
    "#         random.shuffle(words)\n",
    "#         new_sentences.append(' '.join(words))\n",
    "#     new_sentences = list(set(new_sentences))\n",
    "#     return new_sentences\n",
    "\n",
    "# a = augment(str(aug),10)\n",
    "# print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: Synonym replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk import word_tokenize\n",
    "# from nltk.corpus import stopwords\n",
    "\n",
    "# stoplist = stopwords.words('english')\n",
    "\n",
    "\n",
    "# def get_synonyms_lexicon(path):\n",
    "#     synonyms_lexicon = {}\n",
    "#     text_entries = [l.strip() for l in open(path, errors='ignore').readlines()]\n",
    "#     for e in text_entries:\n",
    "#         e = e.split(' ')\n",
    "#         k = e[0]\n",
    "#         v = e[1:len(e)]\n",
    "#         synonyms_lexicon[k] = v\n",
    "#     return synonyms_lexicon\n",
    "\n",
    "\n",
    "# def synonym_replacement(sentence, synonyms_lexicon):\n",
    "#     keys = synonyms_lexicon.keys()\n",
    "#     words = word_tokenize(sentence)\n",
    "#     n_sentence = sentence\n",
    "#     for w in words:\n",
    "#         if w not in stoplist:\n",
    "#             if w in keys:\n",
    "#                 n_sentence = n_sentence.replace(w, synonyms_lexicon[w][0])  # we replace with the first synonym\n",
    "#     return n_sentence\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     text = 'Many customers initiated a return process of the product as it was not suitable for use.' \\\n",
    "#            'It was conditioned in very thin box which caused scratches on the main screen.' \\\n",
    "#            'The involved firms positively answered their clients who were fully refunded.'\n",
    "#     sentences = text.split('.')\n",
    "#     sentences.remove('')\n",
    "#     print(sentences)\n",
    "#     synonyms_lexicon = get_synonyms_lexicon('./ppdb-2.0-xl-all.gz')\n",
    "#     for sentence in sentences:\n",
    "#         new_sentence = synonym_replacement(sentence, synonyms_lexicon)\n",
    "#         print('%s' % sentence)\n",
    "#         print('%s' % new_sentence)\n",
    "#         print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "[1]: https://stackoverflow.com/questions/50686217/keras-how-is-accuracy-calculated-for-multi-label-classification?fbclid=IwAR2yGzEwT3yK6ODkPxVHzUCYvgJgohmMEP0ZeFY_4wxMfCDUsPOtMOfNS_o\n",
    "\n",
    "[2]: https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Accuracy\n",
    "\n",
    "[3]: https://peltarion.com/knowledge-center/documentation/modeling-view/build-an-ai-model/loss-functions/binary-crossentropy\n",
    "\n",
    "[4]: https://github.com/sagr4019/ResearchProject/wiki/Keras-accuracy-(metrics)\n",
    "\n",
    "[5]: https://www.tensorflow.org/api_docs/python/tf/keras/metrics/CategoricalAccuracy\n",
    "\n",
    "[6]: https://www.tensorflow.org/api_docs/python/tf/keras/metrics/BinaryAccuracy\n",
    "\n",
    "[7]: https://datascience.stackexchange.com/questions/14415/how-does-keras-calculate-accuracy\n",
    "\n",
    "[8]: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html\n",
    "\n",
    "[9]: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html\n",
    "\n",
    "[10]: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\n",
    "\n",
    "[11]: https://medium.com/towards-artificial-intelligence/keras-for-multi-label-text-classification-86d194311d0e\n",
    "\n",
    "[12]: https://medium.com/opla/text-augmentation-for-machine-learning-tasks-how-to-grow-your-text-dataset-for-classification-38a9a207f88d\n",
    "\n",
    "[13]: https://github.com/makcedward/nlpaug#augmenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
